{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Program\n",
      "Arshan Ahmad- 18BCS075\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Program\")\n",
    "print(\"Arshan Ahmad- 18BCS075\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'no', 'no', 'yes', 'y \\n']\n",
      "['no', 'no', 'no', 'no', 'n\\n']\n",
      "['no', 'no', 'yes', 'no', 'n\\n']\n",
      "['yes', 'no', 'yes', 'no', 'y\\n']\n",
      "['no', 'no', 'sometimes', 'yes', 'n\\n']\n",
      "['no', 'no', 'no', 'yes', 'n\\n']\n",
      "['yes', 'yes', 'no', 'yes', 'y\\n']\n",
      "['no', 'yes', 'no', 'yes', 'n\\n']\n",
      "['yes', 'no', 'no', 'yes', 'y\\n']\n",
      "['yes', 'no', 'yes', 'no', 'n\\n']\n",
      "['no', 'no', 'sometimes', 'yes', 'n\\n']\n",
      "['no', 'no', 'sometimes', 'yes', 'n\\n']\n",
      "['yes', 'no', 'no', 'yes', 'y\\n']\n",
      "['no', 'no', 'yes', 'no', 'n\\n']\n",
      "['no', 'no', 'sometimes', 'yes', 'n\\n']\n",
      "['no', 'no', 'no', 'yes', 'n\\n']\n",
      "['no', 'no', 'no', 'yes', 'y\\n']\n",
      "['no', 'yes', 'no', 'yes', 'y']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "file=open('decision_tree.csv')\n",
    "for line in file: \n",
    "  line=line.split(',')\n",
    "  print(line)\n",
    "#   for i in range(1,len(line)):\n",
    "#     print(line[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_file(): \n",
    "    with open(\"decision_tree.csv\") as f:\n",
    "        data=f.read()\n",
    "        rows=data.split('\\n')\n",
    "        print(rows)\n",
    "        birth=[]\n",
    "        fly=[]\n",
    "        legs=[]\n",
    "        water=[]\n",
    "        classification=[]\n",
    "        # for row in rows:\n",
    "        #     birth.append(row.split(',')[1])\n",
    "        #     fly.append(row.split(',')[2])\n",
    "        #     legs.append(row.split(',')[4])\n",
    "        #     water.append(row.split(',')[3])\n",
    "        #     classification.append(row.split(',')[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[set(), {'\\n'}]\n"
     ]
    }
   ],
   "source": [
    "file = open('decision_tree.csv')#change it\n",
    "lines =file.readlines()\n",
    "x=[]\n",
    "p=[set() for i in range(len(lines[0].split(' ')))]\n",
    "pd=[{} for i in range(len(lines[0].split(' ')))]\n",
    "for line in lines:\n",
    "  line=line.split(' ')\n",
    "  y=[]\n",
    "  k=0\n",
    "  for i in range(1,len(line)): #change it \n",
    "    p[i].add(line[i])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_total_entropy(train_data, label, class_list):\n",
    "    total_row = train_data.shape[0] #the total size of the dataset\n",
    "    total_entr = 0\n",
    "    \n",
    "    for c in class_list: #for each class in the label\n",
    "        total_class_count = train_data[train_data[label] == c].shape[0] #number of the class\n",
    "        total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) #entropy of the class\n",
    "        total_entr += total_class_entr #adding the class entropy to the total entropy of the dataset\n",
    "    \n",
    "    return total_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(feature_value_data, label, class_list):\n",
    "    class_count = feature_value_data.shape[0]\n",
    "    entropy = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        label_class_count = feature_value_data[feature_value_data[label] == c].shape[0] #row count of class c \n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:\n",
    "            probability_class = label_class_count/class_count #probability of the class\n",
    "            entropy_class = - probability_class * np.log2(probability_class)  #entropy\n",
    "        entropy += entropy_class\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_info_gain(feature_name, train_data, label, class_list):\n",
    "    feature_value_list = train_data[feature_name].unique() #unqiue values of the feature\n",
    "    total_row = train_data.shape[0]\n",
    "    feature_info = 0.0\n",
    "    \n",
    "    for feature_value in feature_value_list:\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value] #filtering rows with that feature_value\n",
    "        feature_value_count = feature_value_data.shape[0]\n",
    "        feature_value_entropy = calc_entropy(feature_value_data, label, class_list) #calculcating entropy for the feature value\n",
    "        feature_value_probability = feature_value_count/total_row\n",
    "        feature_info += feature_value_probability * feature_value_entropy #calculating information of the feature value\n",
    "        \n",
    "    return calc_total_entropy(train_data, label, class_list) - feature_info #calculating information gain by subtracting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_informative_feature(train_data, label, class_list):\n",
    "    feature_list = train_data.columns.drop(label) #finding the feature names in the dataset\n",
    "                                            #N.B. label is not a feature, so dropping it\n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "    \n",
    "    for feature in feature_list:  #for each feature in the dataset\n",
    "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
    "        if max_info_gain < feature_info_gain: #selecting feature name with highest information gain\n",
    "            max_info_gain = feature_info_gain\n",
    "            max_info_feature = feature\n",
    "            \n",
    "    return max_info_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_tree(feature_name, train_data, label, class_list):\n",
    "    feature_value_count_dict = train_data[feature_name].value_counts(sort=False) #dictionary of the count of unqiue feature value\n",
    "    tree = {} #sub tree or node\n",
    "    \n",
    "    for feature_value, count in feature_value_count_dict.iteritems():\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value] #dataset with only feature_name = feature_value\n",
    "        \n",
    "        assigned_to_node = False #flag for tracking feature_value is pure class or not\n",
    "        for c in class_list: #for each class\n",
    "            class_count = feature_value_data[feature_value_data[label] == c].shape[0] #count of class c\n",
    "\n",
    "            if class_count == count: #count of feature_value = count of class (pure class)\n",
    "                tree[feature_value] = c #adding node to the tree\n",
    "                train_data = train_data[train_data[feature_name] != feature_value] #removing rows with feature_value\n",
    "                assigned_to_node = True\n",
    "        if not assigned_to_node: #not pure class\n",
    "            tree[feature_value] = \"?\" #should extend the node, so the branch is marked with ?\n",
    "            \n",
    "    return tree, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(root, prev_feature_value, train_data, label, class_list):\n",
    "    if train_data.shape[0] != 0: #if dataset becomes enpty after updating\n",
    "        max_info_feature = find_most_informative_feature(train_data, label, class_list) #most informative feature\n",
    "        tree, train_data = generate_sub_tree(max_info_feature, train_data, label, class_list) #getting tree node and updated dataset\n",
    "        next_root = None\n",
    "        \n",
    "        if prev_feature_value != None: #add to intermediate node of the tree\n",
    "            root[prev_feature_value] = dict()\n",
    "            root[prev_feature_value][max_info_feature] = tree\n",
    "            next_root = root[prev_feature_value][max_info_feature]\n",
    "        else: #add to root of the tree\n",
    "            root[max_info_feature] = tree\n",
    "            next_root = root[max_info_feature]\n",
    "        \n",
    "        for node, branch in list(next_root.items()): #iterating the tree node\n",
    "            if branch == \"?\": #if it is expandable\n",
    "                feature_value_data = train_data[train_data[max_info_feature] == node] #using the updated dataset\n",
    "                make_tree(next_root, node, feature_value_data, label, class_list) #recursive call with updated dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(train_data_m, label):\n",
    "    train_data = train_data_m.copy() #getting a copy of the dataset\n",
    "    tree = {} #tree which will be updated\n",
    "    class_list = train_data[label].unique() #getting unqiue classes of the label\n",
    "    make_tree(tree, None, train_data_m, label, class_list) #start calling recursion\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global tree\n",
    "# x=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, instance):\n",
    "    print(\"abc\")\n",
    "    print(instance)\n",
    "    if not isinstance(tree, dict): #if it is leaf node\n",
    "        return tree #return the value\n",
    "    else:\n",
    "        root_node = next(iter(tree)) #getting first key/feature name of the dictionary\n",
    "        feature_value = instance[root_node] #value of the feature\n",
    "        if feature_value in tree[root_node]: #checking the feature value in current tree node\n",
    "            return predict(tree[root_node][feature_value], instance) #goto next feature\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(tree, test_data_m, label):\n",
    "#     correct_preditct = 0\n",
    "#     wrong_preditct = 0\n",
    "#     for index, row in test_data_m.iterrows(): #for each row in the dataset\n",
    "#         result = predict(tree, test_data_m.iloc[index]) #predict the row\n",
    "#         if result == test_data_m[label].iloc[index]: #predicted value and expected value is same or not\n",
    "#             correct_preditct += 1 #increase correct count\n",
    "#         else:\n",
    "#             wrong_preditct += 1 #increase incorrect count\n",
    "#     accuracy = correct_preditct / (correct_preditct + wrong_preditct) #calculating accuracy\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tree, test_data_m, label):\n",
    "    correct_preditct = 0\n",
    "    wrong_preditct = 0\n",
    "    for index, row in test_data_m.iterrows(): #for each row in the dataset\n",
    "        result = predict(tree, test_data_m.iloc[index]) #predict the row\n",
    "        if result == test_data_m[label].iloc[index]: #predicted value and expected value is same or not\n",
    "            correct_preditct += 1 #increase correct count\n",
    "        else:\n",
    "            wrong_preditct += 1 #increase incorrect count\n",
    "    accuracy = correct_preditct / (correct_preditct + wrong_preditct) #calculating accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = id3(train_data_m, 'class')\n",
    "accuracy = evaluate(tree, test_data_m, 'class') #evaluating the test datasetprint(tree)\n",
    "print(accuracy)\n",
    "# x =predict(tree,test_data_m.iloc[3])\n",
    "# print(x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
